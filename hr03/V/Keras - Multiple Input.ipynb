{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing with embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87951, 3) (10000, 3)\n",
      "Náklaďáku praskla pneumatika, převrátil se a zasypal silnici pískem => zpr\n",
      "Nástupkyně Natalie. Novinářka Suková zkusí navázat na tenisovou dynastii => spo\n",
      "87951 items - 6.076MB (training set)\n",
      "10000 items - 0.691MB (test set)\n"
     ]
    }
   ],
   "source": [
    "#filename = r'C:\\P\\Machine Learning\\cotoctes\\ucici.csv'\n",
    "filename = r'C:\\P\\Machine Learning\\cotoctes\\ucici_full.csv'\n",
    "import pandas as pd\n",
    "#training = pd.read_csv(filename,delimiter='|',header=None,names = ['cat', 'data'])\n",
    "training = pd.read_csv(filename,delimiter='|',header=None,names = ['cat','other', 'data'])\n",
    "#training['yr'] = training['other'].str.slice(1, 3)\n",
    "#training = training[training['yr'].isin(['15','16','17'])]\n",
    "X_train,X_test = training[0:-10000],training[-10000:]\n",
    "#X_train,X_test = training[0:-100],training[-100:]\n",
    "\n",
    "print(X_train.shape,X_test.shape)\n",
    "for a, b in zip(X_train.data[0:2], X_train.cat[0:2]):\n",
    "    print(\"{0:s} => {1:s}\".format(a,b))\n",
    "    \n",
    "def size_mb(docs):\n",
    "    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
    "\n",
    "data_train_size_mb = size_mb(X_train.data)\n",
    "data_test_size_mb = size_mb(X_test.data)\n",
    "\n",
    "print(\"%d items - %0.3fMB (training set)\" % (\n",
    "    len(X_train.data), data_train_size_mb))\n",
    "print(\"%d items - %0.3fMB (test set)\" % (\n",
    "    len(X_test.data), data_test_size_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max title legth 39\n"
     ]
    }
   ],
   "source": [
    "max_title_legth = X_train['data'].str.split().str.len().max()\n",
    "print('Max title legth: {0}'.format(max_title_legth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max title legth: 255\n"
     ]
    }
   ],
   "source": [
    "max_title_char_legth = X_train['data'].str.len().max()\n",
    "print('Max title legth: {0}'.format(max_title_char_legth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 98534 unique tokens.\n",
      "Shape of data tensor: (87951, 39)\n",
      "[[    0     0     0 ...,  7886   749 49176]\n",
      " [    0     0     0 ...,     2  9005 49177]\n",
      " [    0     0     0 ...,     2   160  1236]\n",
      " ..., \n",
      " [    0     0     0 ...,  4761 48673  1295]\n",
      " [    0     0     0 ...,   647    29  1654]\n",
      " [    0     0     0 ...,     4  1900   410]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\v.dekanovsky\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from keras.utils import to_categorical\n",
    "\n",
    "max_title_legth\n",
    "embedding_vecor_length = 10\n",
    "vocab_size = 80000\n",
    "\n",
    "#num_words is tne number of unique words in the sequence, if there's more top count words are taken\n",
    "tokenizer = Tokenizer(vocab_size)\n",
    "tokenizer.fit_on_texts(X_train['data'])\n",
    "sequences = tokenizer.texts_to_sequences(X_train['data'])\n",
    "sequencesTest = tokenizer.texts_to_sequences(X_test['data'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "input_dim = len(word_index) + 1\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "#max_review_length is the maximum length of the input text so that we can create vector [... 0,0,1,3,50] where 1,3,50 are individual words\n",
    "data = pad_sequences(sequences, max_title_legth)\n",
    "dataTest = pad_sequences(sequencesTest, max_title_legth)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print(data)\n",
    "X_train['sequence'] = data.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataTest[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorize Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "Found 144 unique tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0, 42, 18, 12,  8,  4, 75, 18, 12, 13,  1, 16,  9,  4, 10, 12,  8,\n",
       "         4,  1, 16,  5,  2, 13, 17,  4,  7,  6, 12,  4, 27,  1, 16, 26,  2,\n",
       "        11,  9, 18,  7,  6,  8,  1, 10,  2,  1,  4,  1, 21,  4, 10, 22, 16,\n",
       "         4,  8,  1, 10,  6,  8,  5,  6, 19,  6,  1, 16, 15, 10, 12,  2, 17],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 42, 18, 10,  7,\n",
       "        13, 16, 12, 22,  5, 25,  1, 42,  4,  7,  4,  8,  6,  2, 33,  1, 42,\n",
       "         3, 11,  6,  5, 18, 26, 12,  4,  1, 35, 13, 12,  3, 11, 18,  1, 21,\n",
       "        12, 13, 10, 15,  1,  5,  4, 11, 18, 21,  4,  7,  1,  5,  4,  1,  7,\n",
       "         2,  5,  6, 10,  3, 11,  3, 13,  1, 14, 22,  5,  4, 10,  7,  6,  6]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from keras.utils import to_categorical\n",
    "\n",
    "max_title_char_legth = X_train['data'].str.len().max()\n",
    "print(max_title_char_legth)\n",
    "embedding_vecor_length = 10\n",
    "vocab_size_char = 150\n",
    "\n",
    "#num_words is tne number of unique words in the sequence, if there's more top count words are taken\n",
    "tokenizerChar = Tokenizer(num_words=vocab_size,char_level=True)\n",
    "tokenizerChar.fit_on_texts(X_train['data'])\n",
    "sequences = tokenizerChar.texts_to_sequences(X_train['data'])\n",
    "sequencesTest = tokenizerChar.texts_to_sequences(X_test['data'])\n",
    "\n",
    "word_index = tokenizerChar.word_index\n",
    "input_dim = len(word_index) + 1\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "#max_review_length is the maximum length of the input text so that we can create vector [... 0,0,1,3,50] where 1,3,50 are individual words\n",
    "datachar = pad_sequences(sequences, max_title_char_legth)\n",
    "datacharTest = pad_sequences(sequencesTest, max_title_char_legth)\n",
    "\n",
    "datachar[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.]]\n",
      "(87951, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(X_train.cat)\n",
    "y_train_le = le.transform(X_train.cat)\n",
    "y_test_le = le.transform(X_test.cat)\n",
    "y_train = keras.utils.to_categorical(y_train_le,4)\n",
    "y_test = keras.utils.to_categorical(y_test_le,4)\n",
    "print(y_train[0:4])\n",
    "print(y_train.shape)\n",
    "#list(le.inverse_transform(y_train[0:2]))\n",
    "print(type(y_train_le))\n",
    "print(y_train[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Input model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_20 (InputLayer)            (None, 255)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_19 (InputLayer)            (None, 39)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_21 (Embedding)         (None, 255, 10)       1510        input_20[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "embedding_20 (Embedding)         (None, 39, 50)        4000050     input_19[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_69 (Conv1D)               (None, 253, 32)       992         embedding_21[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_70 (Conv1D)               (None, 251, 32)       1632        embedding_21[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_71 (Conv1D)               (None, 249, 32)       2272        embedding_21[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_66 (Conv1D)               (None, 38, 32)        3232        embedding_20[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_67 (Conv1D)               (None, 37, 32)        4832        embedding_20[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_68 (Conv1D)               (None, 35, 32)        8032        embedding_20[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling1D)  (None, 126, 32)       0           conv1d_69[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling1D)  (None, 125, 32)       0           conv1d_70[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling1D)  (None, 124, 32)       0           conv1d_71[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling1D)  (None, 19, 32)        0           conv1d_66[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling1D)  (None, 18, 32)        0           conv1d_67[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling1D)  (None, 17, 32)        0           conv1d_68[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)     (None, 375, 32)       0           max_pooling1d_69[0][0]           \n",
      "                                                                   max_pooling1d_70[0][0]           \n",
      "                                                                   max_pooling1d_71[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)     (None, 54, 32)        0           max_pooling1d_66[0][0]           \n",
      "                                                                   max_pooling1d_67[0][0]           \n",
      "                                                                   max_pooling1d_68[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)     (None, 429, 32)       0           concatenate_29[0][0]             \n",
      "                                                                   concatenate_28[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_72 (Conv1D)               (None, 428, 64)       4160        concatenate_30[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)             (None, 428, 64)       0           conv1d_72[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_72 (MaxPooling1D)  (None, 214, 64)       0           dropout_14[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)              (None, 13696)         0           max_pooling1d_72[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_16 (Dense)                 (None, 4)             54788       flatten_9[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 4,081,500\n",
      "Trainable params: 4,081,500\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation,Embedding, Flatten,Conv1D,MaxPooling1D, LSTM, Input, concatenate\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "#words\n",
    "inputsWord = Input(shape=(max_title_legth,))\n",
    "xWord = Embedding(vocab_size+1, 50)(inputsWord)\n",
    "x1Word = Conv1D(filters=32, kernel_size=2, activation='relu')(xWord)\n",
    "x1Word = MaxPooling1D(2)(x1Word)\n",
    "x2Word = Conv1D(filters=32, kernel_size=3, activation='relu')(xWord)\n",
    "x2Word = MaxPooling1D(2)(x2Word)\n",
    "x3Word = Conv1D(filters=32, kernel_size=5, activation='relu')(xWord)\n",
    "x3Word = MaxPooling1D(2)(x3Word)\n",
    "xWords = concatenate([x1Word,x2Word,x3Word], axis=1)\n",
    "\n",
    "#chars\n",
    "inputsChar = Input(shape=(max_title_char_legth,))\n",
    "xChar = Embedding(vocab_size_char+1, 10)(inputsChar)\n",
    "x1Char = Conv1D(filters=32, kernel_size=3, activation='relu')(xChar)\n",
    "x1Char = MaxPooling1D(2)(x1Char)\n",
    "x2Char = Conv1D(filters=32, kernel_size=5, activation='relu')(xChar)\n",
    "x2Char = MaxPooling1D(2)(x2Char)\n",
    "x3Char = Conv1D(filters=32, kernel_size=7, activation='relu')(xChar)\n",
    "x3Char = MaxPooling1D(2)(x3Char)\n",
    "xChar = concatenate([x1Char,x2Char,x3Char], axis=1)\n",
    "\n",
    "x = concatenate([xChar,xWords], axis=1)\n",
    "\n",
    "x = Conv1D(filters=64, kernel_size=2, activation='relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = MaxPooling1D(2)(x)\n",
    "x = Flatten()(x)\n",
    "#x = Dense(32, activation='relu')(x)\n",
    "#x = Dropout(0.3)(x)\n",
    "x = Dense(4, activation='softmax')(x)\n",
    "\n",
    "model5 = Model(inputs=[inputsWord,inputsChar], outputs=x)\n",
    "adam = keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model5.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "print(model5.summary())\n",
    "\n",
    "#from keras.utils import plot_model\n",
    "#plot_model(model5, to_file=r'C:\\Test Cell\\model.png')\n",
    "\n",
    "#from IPython.display import SVG\n",
    "#from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "#SVG(model_to_dot(model5).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87951 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "87951/87951 [==============================] - 483s - loss: 0.3403 - acc: 0.8693 - val_loss: 0.2004 - val_acc: 0.9349\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7d06c12da0>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "batch_size = 32\n",
    "model5.fit(x = [data,datachar], y = np.array(y_train), batch_size = batch_size, epochs = 1, validation_data=([dataTest,datacharTest],y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model5.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0   938     8 14266    37    34 35248 42418 10625    20\n",
      "      7    16  4380]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0    25   979  6862  3216  1589  3205   635\n",
      "   1648   776    55]]\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 35  7 13 14  6  2  1 21  1\n",
      "  58  2  9  5  3 24 22  8 13  1  7 11  9 14 15 27  1 31  2  1  9  4 14  6\n",
      "   4 19  2  1 21 11 15 26  4  7 36 17  1  5  2 32 12  3 14 15 33  1 46  8\n",
      "   2  1 23  2  1  7  3  1 16  9  4 11 14  4 59]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 47 14  1 60  4 19  2 24\n",
      "   3  3 12 13  1  5  2 17 36 31  2  7  2  1 28  2 12  4  7  1 31 18 14  5\n",
      "  29  1 10  3 13 12  9  3 17 15 27  1  3 14 17 15  7  8  1 31  4  8  3 24\n",
      "  13  1  4 17  2  9  6 19 12 30  1 10  3 13 14]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.78079575,  0.02268641,  0.12399808,  0.07251977],\n",
       "       [ 0.00708062,  0.0053653 ,  0.55207062,  0.43548355]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real = pd.DataFrame([\"Studie z Černobylu tvrdí, že radiace zvířatům neškodí. Ale je to pravda?\",\"Od Facebooku nemůžete čekat žádné soukromí, odmítl žalobu americký soud\"], columns=['data'])\n",
    "realsequencesWord = tokenizer.texts_to_sequences(real['data'])\n",
    "realsequencesChar = tokenizerChar.texts_to_sequences(real['data'])\n",
    "realWord = pad_sequences(realsequencesWord, max_title_legth)\n",
    "realChar = pad_sequences(realsequencesChar, max_title_char_legth)\n",
    "print(realWord)\n",
    "print(realChar)\n",
    "pred = model5.predict(x = [realWord,realChar])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ona', 'tec']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[le.inverse_transform(t.argmax()) for t in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>res</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Celtic znemožnil v boji o Ligu mistrů Astanu, ...</td>\n",
       "      <td>spo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Katar obnovuje plné diplomatické styky s Íráne...</td>\n",
       "      <td>zpr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Před 100 lety se narodil česko-židovský badate...</td>\n",
       "      <td>tec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pioneer má první interní mechaniku pro přehráv...</td>\n",
       "      <td>tec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Studie z Černobylu tvrdí, že radiace zvířatům ...</td>\n",
       "      <td>ona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Černý pátek je tu. Jak a kde si počíhat na slevy</td>\n",
       "      <td>tec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Karolína Plíšková začne hájit trůn. O číslo 1 ...</td>\n",
       "      <td>spo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Od Facebooku nemůžete čekat žádné soukromí, od...</td>\n",
       "      <td>tec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Oparům se lze bránit. Pomohou antivirotika i p...</td>\n",
       "      <td>ona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vyhovuje nám, že nás předem odepisují, soudí z...</td>\n",
       "      <td>spo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Zbavte se rutinních prací s e-maily. Správné p...</td>\n",
       "      <td>tec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Motocyklista Abraham bude i nadále jezdit za t...</td>\n",
       "      <td>spo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bez bodu, poslední. Sokolovští fotbalisté se p...</td>\n",
       "      <td>spo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Čeští mladíci zdolali domácí Gruzii a jsou v s...</td>\n",
       "      <td>spo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Muž najel u Paříže do lidí na zahrádce restaur...</td>\n",
       "      <td>zpr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TÝDEN OBRAZEM: konec ramadánu, hlava v tlamě k...</td>\n",
       "      <td>zpr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Podvodný e-mail chce jménem exekutora peníze, ...</td>\n",
       "      <td>tec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>První televizor s Windows 10 míří do prodeje</td>\n",
       "      <td>tec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Trump chce vzít peníze vědcům. Univerzity přij...</td>\n",
       "      <td>zpr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Jak se „ruská propaganda“ dostala až do projev...</td>\n",
       "      <td>zpr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Muž se nepohodl s kolegou kvůli pracovní morál...</td>\n",
       "      <td>zpr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Osvoboďte bradavky, hlásá Naomi Campbellová a ...</td>\n",
       "      <td>ona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Zeman se setká s Chovancem, chce se ho zeptat ...</td>\n",
       "      <td>zpr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Software zdarma: Aktivujte virtuální plochy i ...</td>\n",
       "      <td>tec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Ženy mají v kuchyni cit pro detail a systém, ř...</td>\n",
       "      <td>ona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Pět triků, díky kterým při chůzi spálíte mnohe...</td>\n",
       "      <td>ona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>VIDEO: Musel jsem se krotit, říká Nečesaný o s...</td>\n",
       "      <td>zpr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Florbalisté Bulldogs Brno poznali svět, chtějí...</td>\n",
       "      <td>spo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Za výhru nad Spartou špekáčky od řezníka. Fotb...</td>\n",
       "      <td>spo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>100 POHLEDŮ: Blahobytná rodinka na nádraží v O...</td>\n",
       "      <td>zpr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>Facebook „znemožnil“ zneužívání dat ke šmírová...</td>\n",
       "      <td>tec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>Kancelářská sponka v akci: umí udržet culík i ...</td>\n",
       "      <td>ona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>Microsoft koupil deset milionů molekul synteti...</td>\n",
       "      <td>tec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>Lidé jej vraceli zlomený, nyní Microsoft s chy...</td>\n",
       "      <td>tec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>OBRAZEM: Žlutá blond nemusí vypadat lacině, do...</td>\n",
       "      <td>ona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>Tragická nehoda autobusu v Rusku, zemřelo 14 l...</td>\n",
       "      <td>zpr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>Otestovali jsme (ne)ovladatelné hračky. Minidr...</td>\n",
       "      <td>tec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>Proč letadlo malovalo letadlo? Piloti testoval...</td>\n",
       "      <td>tec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>Britští vědci představili jednoduchý kapesní „...</td>\n",
       "      <td>tec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>Před 100 lety si britští králové začali říkat ...</td>\n",
       "      <td>tec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>Nechci to vidět! Po hrůze z Wimbledonu přišla ...</td>\n",
       "      <td>spo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>Záchranná brzda pro PC aneb co pomůže získat z...</td>\n",
       "      <td>tec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>Google opravuje prohlížeč Chrome. Záplaty dost...</td>\n",
       "      <td>tec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>Burnett pokračuje. S Pardubicemi se chce popra...</td>\n",
       "      <td>spo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>Kam se dalo za 180 minut dojet vlakem za císař...</td>\n",
       "      <td>spo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>Žijeme naruby: chceme sex, ale místo něj se vě...</td>\n",
       "      <td>ona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>STAN chce pro učitele plat od 37 tisíc či novo...</td>\n",
       "      <td>zpr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>Apple chce použít drony, aby vylepšil mapy a d...</td>\n",
       "      <td>tec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>Stát nakoupí potraviny pro lidi v nouzi, nově ...</td>\n",
       "      <td>zpr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>Češi jsou v žebříčku FIFA na 40. místě, do čel...</td>\n",
       "      <td>spo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>Jsme stříbrní, utěšují se američtí fotbalisté ...</td>\n",
       "      <td>spo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>Závislost na plastikách mi zničila sexuální ži...</td>\n",
       "      <td>ona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>Internet, jak jej známe, může skončit, varoval...</td>\n",
       "      <td>tec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>V případě řízení s firmou FAU mohla být poruše...</td>\n",
       "      <td>zpr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Pokrok v IT je obrovský, ale skrytý, řekl šéf ...</td>\n",
       "      <td>tec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Cítím se jako junior, těší Federera. A vyhlíží...</td>\n",
       "      <td>spo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Google ukončí vývoj oblíbeného editoru Picasa,...</td>\n",
       "      <td>tec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Spotify hlásí 100 milionů uživatelů měsíčně. A...</td>\n",
       "      <td>tec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Software zdarma: Windows vždy ve skvělém stavu...</td>\n",
       "      <td>tec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Vyzkoušela jsem mraky diet, říká zpěvačka Ilon...</td>\n",
       "      <td>ona</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  data  res\n",
       "0    Celtic znemožnil v boji o Ligu mistrů Astanu, ...  spo\n",
       "1    Katar obnovuje plné diplomatické styky s Íráne...  zpr\n",
       "2    Před 100 lety se narodil česko-židovský badate...  tec\n",
       "3    Pioneer má první interní mechaniku pro přehráv...  tec\n",
       "4    Studie z Černobylu tvrdí, že radiace zvířatům ...  ona\n",
       "5     Černý pátek je tu. Jak a kde si počíhat na slevy  tec\n",
       "6    Karolína Plíšková začne hájit trůn. O číslo 1 ...  spo\n",
       "7    Od Facebooku nemůžete čekat žádné soukromí, od...  tec\n",
       "8    Oparům se lze bránit. Pomohou antivirotika i p...  ona\n",
       "9    Vyhovuje nám, že nás předem odepisují, soudí z...  spo\n",
       "10   Zbavte se rutinních prací s e-maily. Správné p...  tec\n",
       "11   Motocyklista Abraham bude i nadále jezdit za t...  spo\n",
       "12   Bez bodu, poslední. Sokolovští fotbalisté se p...  spo\n",
       "13   Čeští mladíci zdolali domácí Gruzii a jsou v s...  spo\n",
       "14   Muž najel u Paříže do lidí na zahrádce restaur...  zpr\n",
       "15   TÝDEN OBRAZEM: konec ramadánu, hlava v tlamě k...  zpr\n",
       "16   Podvodný e-mail chce jménem exekutora peníze, ...  tec\n",
       "17        První televizor s Windows 10 míří do prodeje  tec\n",
       "18   Trump chce vzít peníze vědcům. Univerzity přij...  zpr\n",
       "19   Jak se „ruská propaganda“ dostala až do projev...  zpr\n",
       "20   Muž se nepohodl s kolegou kvůli pracovní morál...  zpr\n",
       "21   Osvoboďte bradavky, hlásá Naomi Campbellová a ...  ona\n",
       "22   Zeman se setká s Chovancem, chce se ho zeptat ...  zpr\n",
       "23   Software zdarma: Aktivujte virtuální plochy i ...  tec\n",
       "24   Ženy mají v kuchyni cit pro detail a systém, ř...  ona\n",
       "25   Pět triků, díky kterým při chůzi spálíte mnohe...  ona\n",
       "26   VIDEO: Musel jsem se krotit, říká Nečesaný o s...  zpr\n",
       "27   Florbalisté Bulldogs Brno poznali svět, chtějí...  spo\n",
       "28   Za výhru nad Spartou špekáčky od řezníka. Fotb...  spo\n",
       "29   100 POHLEDŮ: Blahobytná rodinka na nádraží v O...  zpr\n",
       "..                                                 ...  ...\n",
       "970  Facebook „znemožnil“ zneužívání dat ke šmírová...  tec\n",
       "971  Kancelářská sponka v akci: umí udržet culík i ...  ona\n",
       "972  Microsoft koupil deset milionů molekul synteti...  tec\n",
       "973  Lidé jej vraceli zlomený, nyní Microsoft s chy...  tec\n",
       "974  OBRAZEM: Žlutá blond nemusí vypadat lacině, do...  ona\n",
       "975  Tragická nehoda autobusu v Rusku, zemřelo 14 l...  zpr\n",
       "976  Otestovali jsme (ne)ovladatelné hračky. Minidr...  tec\n",
       "977  Proč letadlo malovalo letadlo? Piloti testoval...  tec\n",
       "978  Britští vědci představili jednoduchý kapesní „...  tec\n",
       "979  Před 100 lety si britští králové začali říkat ...  tec\n",
       "980  Nechci to vidět! Po hrůze z Wimbledonu přišla ...  spo\n",
       "981  Záchranná brzda pro PC aneb co pomůže získat z...  tec\n",
       "982  Google opravuje prohlížeč Chrome. Záplaty dost...  tec\n",
       "983  Burnett pokračuje. S Pardubicemi se chce popra...  spo\n",
       "984  Kam se dalo za 180 minut dojet vlakem za císař...  spo\n",
       "985  Žijeme naruby: chceme sex, ale místo něj se vě...  ona\n",
       "986  STAN chce pro učitele plat od 37 tisíc či novo...  zpr\n",
       "987  Apple chce použít drony, aby vylepšil mapy a d...  tec\n",
       "988  Stát nakoupí potraviny pro lidi v nouzi, nově ...  zpr\n",
       "989  Češi jsou v žebříčku FIFA na 40. místě, do čel...  spo\n",
       "990  Jsme stříbrní, utěšují se američtí fotbalisté ...  spo\n",
       "991  Závislost na plastikách mi zničila sexuální ži...  ona\n",
       "992  Internet, jak jej známe, může skončit, varoval...  tec\n",
       "993  V případě řízení s firmou FAU mohla být poruše...  zpr\n",
       "994  Pokrok v IT je obrovský, ale skrytý, řekl šéf ...  tec\n",
       "995  Cítím se jako junior, těší Federera. A vyhlíží...  spo\n",
       "996  Google ukončí vývoj oblíbeného editoru Picasa,...  tec\n",
       "997  Spotify hlásí 100 milionů uživatelů měsíčně. A...  tec\n",
       "998  Software zdarma: Windows vždy ve skvělém stavu...  tec\n",
       "999  Vyzkoušela jsem mraky diet, říká zpěvačka Ilon...  ona\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = r'C:\\P\\Machine Learning\\cotoctes\\testovaci.csv'\n",
    "testing = pd.read_csv(filename,delimiter='|',header=None,names = ['data'])\n",
    "testingsequencesWord = tokenizer.texts_to_sequences(testing['data'])\n",
    "testingsequencesChar = tokenizerChar.texts_to_sequences(testing['data'])\n",
    "testingWord = pad_sequences(testingsequencesWord, max_title_legth)\n",
    "testingChar = pad_sequences(testingsequencesChar, max_title_char_legth)\n",
    "tr = model5.predict([testingWord,testingChar])\n",
    "print(len(tr))\n",
    "tres = [le.inverse_transform(t.argmax()) for t in tr]\n",
    "tres[0:4]\n",
    "testing['res'] = tres\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(r'C:\\P\\Machine Learning\\cotoctes\\outputkeras2.csv','w') as f:\n",
    "    wr = csv.writer(f, delimiter='\\n')\n",
    "    wr.writerow(tres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model5.save(r'C:\\P\\Machine Learning\\cotoctes\\904')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
