{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4)\n",
      "[[1 1 0 1]\n",
      " [0 1 1 1]]\n",
      "['co', 'je', 'něco', 'to']\n",
      "{'tokenizer': None, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'preprocessor': None, 'analyzer': 'word', 'input': 'content', 'ngram_range': (1, 1), 'min_df': 1, 'strip_accents': None, 'encoding': 'utf-8', 'vocabulary': None, 'stop_words': None, 'max_df': 1.0, 'max_features': None, 'dtype': <class 'numpy.int64'>, 'decode_error': 'strict', 'lowercase': True, 'binary': False}\n"
     ]
    }
   ],
   "source": [
    "textSamples = ['Co to je','To je něco']\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(textSamples) \n",
    "print(X_train_counts.shape)\n",
    "print(X_train_counts.toarray())\n",
    "print(count_vect.get_feature_names())\n",
    "print(count_vect.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1]\n",
      " [0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_counts.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NWordGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 0 1 1]\n",
      " [0 0 1 1 1 1 1]]\n",
      "['co', 'co to', 'je', 'je něco', 'něco', 'to', 'to je']\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2),analyzer='word')\n",
    "X_cv = cv.fit_transform(textSamples)\n",
    "print(X_cv.toarray())\n",
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 2, 1, 1, 2, 2]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/36046180/how-to-selected-vocabulary-in-scikit-countvectorizer\n",
    "term_freq_matrix = X_cv.sum(0)\n",
    "term_freq_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "min_freq = np.amin(term_freq_matrix)\n",
    "min_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['co', 'co to', 'je', 'je něco', 'něco', 'to', 'to je']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCharGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 2 2 1 1 1 1 1 0 0 0]\n",
      " [2 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 2 1 1 0 1 1 1 1 1 1]] (2, 29)\n"
     ]
    }
   ],
   "source": [
    "cvch = CountVectorizer(ngram_range=(1,3),analyzer='char')\n",
    "X_cvch = cvch.fit_transform(textSamples)\n",
    "print(X_cvch.toarray(),X_cvch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " ' j',\n",
       " ' je',\n",
       " ' n',\n",
       " ' ně',\n",
       " ' t',\n",
       " ' to',\n",
       " 'c',\n",
       " 'co',\n",
       " 'co ',\n",
       " 'e',\n",
       " 'e ',\n",
       " 'e n',\n",
       " 'j',\n",
       " 'je',\n",
       " 'je ',\n",
       " 'n',\n",
       " 'ně',\n",
       " 'něc',\n",
       " 'o',\n",
       " 'o ',\n",
       " 'o j',\n",
       " 'o t',\n",
       " 't',\n",
       " 'to',\n",
       " 'to ',\n",
       " 'ě',\n",
       " 'ěc',\n",
       " 'ěco']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvch.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline & Feature Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply pipeline of steps by Pipeline\n",
    "Concatenate various inputs into single array using Feature Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to scikit-learn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdText = pd.DataFrame({'text':textSamples})\n",
    "clf = FeatureUnion(\n",
    "        transformer_list=[\n",
    "    ('word', Pipeline([\n",
    "                    ('wordGrams', ItemSelector(key='text')),\n",
    "                    ('vect', CountVectorizer(analyzer='word',ngram_range=(1,1))),\n",
    "                   \n",
    "                       ])),\n",
    "    ('chars', Pipeline([\n",
    "                    ('nGrams', ItemSelector(key='text')),\n",
    "                    ('vect', CountVectorizer(analyzer='char',ngram_range=(1,3))),\n",
    "                    \n",
    "        ])),\n",
    "    ],\n",
    ")\n",
    "clf_model = clf.fit(pdText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 33)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_model.transform(pdText).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 2, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_model.transform(pdText).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 20)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vectorizer = HashingVectorizer(n_features=20,norm='l1',ngram_range= (1, 1), analyzer= 'word');\n",
    "#X_test = vectorizer.transform(pdText)\n",
    "X_test = vectorizer.fit_transform(['Co to je','Co to není','Co to co to','Samuel zasel do sklepa pro chleba'])\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.33333333,  0.        ,  0.        , -0.33333333,\n",
       "         0.        ,  0.        ,  0.        ,  0.33333333,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.33333333,  0.        ,  0.        ,  0.        ,\n",
       "        -0.33333333,  0.        ,  0.        ,  0.33333333,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.5       ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.5       ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.16666667,  0.        ,  0.        ,\n",
       "         0.        ,  0.        , -0.33333333, -0.16666667,  0.        ,\n",
       "         0.        ,  0.16666667,  0.        ,  0.16666667,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alternate_sign': True,\n",
       " 'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.float64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'n_features': 20,\n",
       " 'ngram_range': (1, 1),\n",
       " 'non_negative': False,\n",
       " 'norm': 'l1',\n",
       " 'preprocessor': None,\n",
       " 'stop_words': None,\n",
       " 'strip_accents': None,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import Perceptron\n",
    "pipelinek = Pipeline([\n",
    "                   ('union', FeatureUnion(\n",
    "                           transformer_list=[\n",
    "                       ('words', CountVectorizer(analyzer='word',ngram_range=(1,2), min_df = 2, max_features =10000)),\n",
    "                       #('chars', CountVectorizer(analyzer='char',ngram_range=(1,3), min_df = 2, max_features =10000)),\n",
    "                        ],)),\n",
    "                   ('tfidf', TfidfTransformer(use_idf=False)),   \n",
    "                    ])\n",
    "X_trainVect = pipelinek.fit_transform(pdText)\n",
    "X_trainVect.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdText.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         text\n",
      "0    Co to je\n",
      "1  To je něco\n"
     ]
    }
   ],
   "source": [
    "print(pdText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Count Vectorizer and Tdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:\n",
      "(7, 5)\n",
      "Sparse metics:\n",
      "  (0, 1)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 0)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 4)\t1\n",
      "  (2, 4)\t2\n",
      "  (2, 0)\t2\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 4)\t1\n",
      "  (4, 1)\t1\n",
      "  (5, 4)\t1\n",
      "  (6, 0)\t1\n",
      "Array:\n",
      "[[1 1 0 0 1]\n",
      " [0 1 0 1 1]\n",
      " [2 0 0 0 2]\n",
      " [0 1 1 0 1]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [1 0 0 0 0]]\n",
      "Vocabulary list: {'je': 1, 'něco': 3, 'to': 4, 'co': 0, 'lev': 2}\n",
      " Length of the vocabulary: 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>CountVectorizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Co to je</td>\n",
       "      <td>[1, 1, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To je něco</td>\n",
       "      <td>[0, 1, 0, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Co to co to</td>\n",
       "      <td>[2, 0, 0, 0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To je lev</td>\n",
       "      <td>[0, 1, 1, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>je</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>to</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>co</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         texts  CountVectorizer\n",
       "0     Co to je  [1, 1, 0, 0, 1]\n",
       "1   To je něco  [0, 1, 0, 1, 1]\n",
       "2  Co to co to  [2, 0, 0, 0, 2]\n",
       "3    To je lev  [0, 1, 1, 0, 1]\n",
       "4           je  [0, 1, 0, 0, 0]\n",
       "5           to  [0, 0, 0, 0, 1]\n",
       "6           co  [1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textSamples = ['Co to je','To je něco','Co to co to','To je lev','je','to','co']\n",
    "texts = pd.DataFrame(textSamples, columns=[\"texts\"])\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "#min_df = 2, max_features =1\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(textSamples) \n",
    "print(\"Training data shape:\")\n",
    "print(X_train_counts.shape)\n",
    "print(\"Sparse metics:\")\n",
    "print(X_train_counts)\n",
    "print(\"Array:\")\n",
    "print(\"{0}\".format(X_train_counts.toarray()))\n",
    "print(\"Vocabulary list: {0}\".format(count_vect.vocabulary_))\n",
    "print(\" Length of the vocabulary: {0}\".format(len(count_vect.vocabulary_)))\n",
    "texts['CountVectorizer'] = list(X_train_counts.toarray())\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False,\n",
       "         use_idf=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = TfidfTransformer(use_idf=False)\n",
    "t.fit(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 5)\n",
      "sparse metric:\n",
      "  (0, 1)\t0.57735026919\n",
      "  (0, 4)\t0.57735026919\n",
      "  (0, 0)\t0.57735026919\n",
      "  (1, 3)\t0.57735026919\n",
      "  (1, 1)\t0.57735026919\n",
      "  (1, 4)\t0.57735026919\n",
      "  (2, 4)\t0.707106781187\n",
      "  (2, 0)\t0.707106781187\n",
      "  (3, 2)\t0.57735026919\n",
      "  (3, 1)\t0.57735026919\n",
      "  (3, 4)\t0.57735026919\n",
      "  (4, 1)\t1.0\n",
      "  (5, 4)\t1.0\n",
      "  (6, 0)\t1.0\n",
      "Array:\n",
      "[[ 0.57735027  0.57735027  0.          0.          0.57735027]\n",
      " [ 0.          0.57735027  0.          0.57735027  0.57735027]\n",
      " [ 0.70710678  0.          0.          0.          0.70710678]\n",
      " [ 0.          0.57735027  0.57735027  0.          0.57735027]\n",
      " [ 0.          1.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          1.        ]\n",
      " [ 1.          0.          0.          0.          0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>CountVectorizer</th>\n",
       "      <th>tfidfs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Co to je</td>\n",
       "      <td>[1, 1, 0, 0, 1]</td>\n",
       "      <td>[0.57735026919, 0.57735026919, 0.0, 0.0, 0.577...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To je něco</td>\n",
       "      <td>[0, 1, 0, 1, 1]</td>\n",
       "      <td>[0.0, 0.57735026919, 0.0, 0.57735026919, 0.577...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Co to co to</td>\n",
       "      <td>[2, 0, 0, 0, 2]</td>\n",
       "      <td>[0.707106781187, 0.0, 0.0, 0.0, 0.707106781187]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To je lev</td>\n",
       "      <td>[0, 1, 1, 0, 1]</td>\n",
       "      <td>[0.0, 0.57735026919, 0.57735026919, 0.0, 0.577...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>je</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>to</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>co</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         texts  CountVectorizer  \\\n",
       "0     Co to je  [1, 1, 0, 0, 1]   \n",
       "1   To je něco  [0, 1, 0, 1, 1]   \n",
       "2  Co to co to  [2, 0, 0, 0, 2]   \n",
       "3    To je lev  [0, 1, 1, 0, 1]   \n",
       "4           je  [0, 1, 0, 0, 0]   \n",
       "5           to  [0, 0, 0, 0, 1]   \n",
       "6           co  [1, 0, 0, 0, 0]   \n",
       "\n",
       "                                              tfidfs  \n",
       "0  [0.57735026919, 0.57735026919, 0.0, 0.0, 0.577...  \n",
       "1  [0.0, 0.57735026919, 0.0, 0.57735026919, 0.577...  \n",
       "2    [0.707106781187, 0.0, 0.0, 0.0, 0.707106781187]  \n",
       "3  [0.0, 0.57735026919, 0.57735026919, 0.0, 0.577...  \n",
       "4                          [0.0, 1.0, 0.0, 0.0, 0.0]  \n",
       "5                          [0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "6                          [1.0, 0.0, 0.0, 0.0, 0.0]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_Tfid = t.transform(X_train_counts)\n",
    "print(X_train_Tfid.shape)\n",
    "print(\"sparse metric:\")\n",
    "print(X_train_Tfid)\n",
    "print(\"Array:\")\n",
    "print(X_train_Tfid.toarray())\n",
    "texts['tfidfs'] = list(X_train_Tfid.toarray())\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.33333333  0.          0.33333333  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "  -0.33333333]\n",
      " [ 0.          0.33333333  0.          0.          0.          0.\n",
      "   0.33333333  0.          0.          0.          0.          0.          0.\n",
      "   0.         -0.33333333]\n",
      " [ 0.          0.5         0.          0.5         0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.33333333  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.33333333  0.          0.          0.          0.\n",
      "  -0.33333333]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "  -1.        ]\n",
      " [ 0.          1.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          1.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>CountVectorizer</th>\n",
       "      <th>tfidfs</th>\n",
       "      <th>Hashing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Co to je</td>\n",
       "      <td>[1, 1, 0, 0, 1]</td>\n",
       "      <td>[0.57735026919, 0.57735026919, 0.0, 0.0, 0.577...</td>\n",
       "      <td>[0.0, 0.333333333333, 0.0, 0.333333333333, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To je něco</td>\n",
       "      <td>[0, 1, 0, 1, 1]</td>\n",
       "      <td>[0.0, 0.57735026919, 0.0, 0.57735026919, 0.577...</td>\n",
       "      <td>[0.0, 0.333333333333, 0.0, 0.0, 0.0, 0.0, 0.33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Co to co to</td>\n",
       "      <td>[2, 0, 0, 0, 2]</td>\n",
       "      <td>[0.707106781187, 0.0, 0.0, 0.0, 0.707106781187]</td>\n",
       "      <td>[0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To je lev</td>\n",
       "      <td>[0, 1, 1, 0, 1]</td>\n",
       "      <td>[0.0, 0.57735026919, 0.57735026919, 0.0, 0.577...</td>\n",
       "      <td>[0.0, 0.333333333333, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>je</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>to</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>co</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         texts  CountVectorizer  \\\n",
       "0     Co to je  [1, 1, 0, 0, 1]   \n",
       "1   To je něco  [0, 1, 0, 1, 1]   \n",
       "2  Co to co to  [2, 0, 0, 0, 2]   \n",
       "3    To je lev  [0, 1, 1, 0, 1]   \n",
       "4           je  [0, 1, 0, 0, 0]   \n",
       "5           to  [0, 0, 0, 0, 1]   \n",
       "6           co  [1, 0, 0, 0, 0]   \n",
       "\n",
       "                                              tfidfs  \\\n",
       "0  [0.57735026919, 0.57735026919, 0.0, 0.0, 0.577...   \n",
       "1  [0.0, 0.57735026919, 0.0, 0.57735026919, 0.577...   \n",
       "2    [0.707106781187, 0.0, 0.0, 0.0, 0.707106781187]   \n",
       "3  [0.0, 0.57735026919, 0.57735026919, 0.0, 0.577...   \n",
       "4                          [0.0, 1.0, 0.0, 0.0, 0.0]   \n",
       "5                          [0.0, 0.0, 0.0, 0.0, 1.0]   \n",
       "6                          [1.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "\n",
       "                                             Hashing  \n",
       "0  [0.0, 0.333333333333, 0.0, 0.333333333333, 0.0...  \n",
       "1  [0.0, 0.333333333333, 0.0, 0.0, 0.0, 0.0, 0.33...  \n",
       "2  [0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.333333333333, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "5  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "6  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vectorizer = HashingVectorizer(n_features=15,norm='l1',ngram_range= (1, 1), analyzer= 'word');\n",
    "#X_test = vectorizer.transform(pdText)\n",
    "X_test = vectorizer.fit_transform(textSamples)\n",
    "print(X_test.toarray())\n",
    "texts['Hashing'] = list(X_test.toarray())\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Keras Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique tokens.\n",
      "Shape of data tensor: (4, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 2, 3],\n",
       "       [0, 0, 0, 1, 2, 4],\n",
       "       [0, 0, 1, 2, 1, 2],\n",
       "       [0, 0, 0, 5, 6, 1]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from keras.utils import to_categorical\n",
    "\n",
    "texts = ['Co to je','Co to není','Co to co to','Samuel zasel do sklepa pro co?']\n",
    "\n",
    "#num_words is tne number of unique words in the sequence\n",
    "tokenizer = Tokenizer(num_words=7)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "#max_len is the maximum length of the input text so that we can create vector [0,0,1,3,50] where 1,3,50 are individual words\n",
    "data = pad_sequences(sequences, maxlen=6)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'není': 1, 'zasel': 1, 'co': 4, 'pro': 1, 'je': 1, 'samuel': 1, 'to': 3, 'sklepa': 1, 'do': 1}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'není': 4, 'zasel': 6, 'co': 1, 'pro': 9, 'je': 3, 'samuel': 5, 'to': 2, 'sklepa': 8, 'do': 7}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('co', 5), ('to', 4), ('je', 1), ('není', 1), ('samuel', 1), ('zasel', 1), ('do', 1), ('sklepa', 1), ('pro', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [1, 2, 4], [1, 2, 1, 2], [5, 6, 1]]\n",
      "[[ 0.  1.  1.  1.  0.  0.  0.]\n",
      " [ 0.  1.  1.  0.  1.  0.  0.]\n",
      " [ 0.  1.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)\n",
    "print(tokenizer.sequences_to_matrix(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.33333333  0.33333333  0.33333333  0.          0.          0.        ]\n",
      " [ 0.          0.33333333  0.33333333  0.          0.33333333  0.          0.        ]\n",
      " [ 0.          0.5         0.5         0.          0.          0.          0.        ]\n",
      " [ 0.          0.33333333  0.          0.          0.          0.33333333\n",
      "   0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.sequences_to_matrix(sequences,mode='freq'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Co to je', 'Co to není', 'Co to co to', 'Samuel zasel do sklepa pro co?']\n",
      "[[5, 2, 1, 4, 2, 1, 12, 3], [5, 2, 1, 4, 2, 1, 8, 3, 8, 13], [5, 2, 1, 4, 2, 1, 9, 2, 1, 4, 2], [14, 6, 15, 16, 3, 7, 1, 17, 6, 10, 3, 7, 1, 18, 2, 1, 10, 19, 7, 3, 11, 6, 1, 11, 20, 2, 1, 9, 2, 21]]\n",
      "{'t': 4, 'S': 14, 'j': 12, 'm': 15, 'o': 2, 'u': 16, 'z': 17, 's': 10, '?': 21, 'í': 13, 'r': 20, 'n': 8, 'c': 9, 'e': 3, 'd': 18, 'p': 11, 'C': 5, 'k': 19, ' ': 1, 'a': 6, 'l': 7}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=100, char_level=True, lower=True)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "print(texts)\n",
    "print(sequences)\n",
    "print(tokenizer.word_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
